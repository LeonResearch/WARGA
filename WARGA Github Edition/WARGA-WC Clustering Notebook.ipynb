{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import optim\n",
    "from model import GCNModelAE, Regularizer\n",
    "from optimizer import loss_function1\n",
    "from utils import load_data, preprocess_graph, get_roc_score, load_data_with_labels\n",
    "from sklearn.cluster import KMeans\n",
    "from metrics import clustering_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Settings\n",
    "\n",
    "Here in node clustering we only use half of the training iterations for link prediction (i.e. 100 epochs for Cora and Citeseer, and 750 epochs for PubMed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
    "# We recommend 100 epochs for Cora and Citeseer, and 800 epochs for PubMed\n",
    "parser.add_argument('--hidden1', type=int, default=32, help='Number of units in the first encoding layer.')\n",
    "parser.add_argument('--hidden2', type=int, default=16, help='Number of units in the second embedding layer.')\n",
    "parser.add_argument('--hidden3', type=int, default=16, help='Number of units in the first hidden layer of Regularizer.')\n",
    "parser.add_argument('--hidden4', type=int, default=64, help='Number of units in the second hidden layer of Regularizer.')\n",
    "parser.add_argument('--clamp', type=float, default=0.01, help='Weight clamp for Regularizer Parameters.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate for Generator.')\n",
    "parser.add_argument('--reglr', type=float, default=0.001, help='Initial learning rate for Regularizer.')\n",
    "parser.add_argument('--dropout', type=float, default=0., help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--dataset-str', type=str, default='cora', help='type of dataset.')\n",
    "\n",
    "args,unknown = parser.parse_known_args()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Node Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae_for(args):\n",
    "    print(\"Using {} dataset\".format(args.dataset_str))\n",
    "    adj, features,true_labels = load_data_with_labels(args.dataset_str)\n",
    "    n_nodes, feat_dim = features.shape\n",
    "    features = features.to(device)\n",
    "    \n",
    "    if args.dataset_str == 'cora':\n",
    "        n_clusters = 7\n",
    "    elif args.dataset_str == 'citeseer':\n",
    "        n_clusters = 6\n",
    "    else:\n",
    "        n_clusters = 3\n",
    "\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "\n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_norm = adj_norm.to(device)\n",
    "    \n",
    "    adj_label = adj + sp.eye(adj.shape[0])\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    adj_label = adj_label.to(device)\n",
    "\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "    model = GCNModelAE(feat_dim, args.hidden1, args.hidden2, args.dropout).to(device)\n",
    "    regularizer = Regularizer(args.hidden3, args.hidden2, args.hidden4).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    regularizer_optimizer = optim.Adam(regularizer.parameters(), lr=args.reglr)\n",
    "    \n",
    "    clustering_scores=[]\n",
    "    for epoch in range(args.epochs):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        regularizer.train() \n",
    "        \n",
    "        #Generate embeddings\n",
    "        predicted_labels_prob, emb = model(features, adj_norm)\n",
    "        \n",
    "        #Wasserstein Regularizer\n",
    "        for i in range(1):\n",
    "            f_z = regularizer(emb).to(device)\n",
    "            r = torch.normal(0.0, 1.0, [n_nodes, args.hidden2]).to(device)\n",
    "            f_r = regularizer(r)          \n",
    "            reg_loss = - f_r.mean() + f_z.mean() \n",
    "            \n",
    "            regularizer_optimizer.zero_grad()\n",
    "            reg_loss.backward(retain_graph=True)\n",
    "            regularizer_optimizer.step()\n",
    "            \n",
    "            # weight clamp\n",
    "            for p in regularizer.parameters():\n",
    "                p.data.clamp_(-args.clamp, args.clamp)\n",
    "        \n",
    "        #GAE Update\n",
    "        f_z = regularizer(emb)  \n",
    "        generator_loss = -f_z.mean()\n",
    "        loss = loss_function1(preds=predicted_labels_prob, labels=adj_label,\n",
    "                             norm=norm, pos_weight=torch.tensor(pos_weight))\n",
    "        loss = loss + generator_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()\n",
    "        if epoch%20==0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss))\n",
    "            print(\"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        \n",
    "    np_emb = emb.cpu().detach().numpy()\n",
    "    kmeans = KMeans(n_clusters= n_clusters, random_state=args.seed).fit(np_emb)\n",
    "    predict_labels = kmeans.predict(np_emb)\n",
    "    cm = clustering_metrics(true_labels, predict_labels)\n",
    "    acc, nmi, f1_macro, precision_macro, adjscore = cm.evaluationClusterModelFromLabel()\n",
    "\n",
    "    clustering_scores.append([acc, nmi, f1_macro, precision_macro, adjscore])\n",
    " \n",
    "    return clustering_scores[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.77257\n",
      "time= 0.47882\n",
      "Epoch: 0021 train_loss= 0.72065\n",
      "time= 0.01695\n",
      "Epoch: 0041 train_loss= 0.58250\n",
      "time= 0.01795\n",
      "Epoch: 0061 train_loss= 0.53101\n",
      "time= 0.01795\n",
      "Epoch: 0081 train_loss= 0.51072\n",
      "time= 0.01795\n",
      "ACC=0.653250, f1_macro=0.629955, precision_macro=0.645058, recall_macro=0.652351, f1_micro=0.653250, precision_micro=0.653250, recall_micro=0.653250, NMI=0.498641, ADJ_RAND_SCORE=0.429786\n",
      "Seed 1\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.76904\n",
      "time= 0.01895\n",
      "Epoch: 0021 train_loss= 0.67864\n",
      "time= 0.01695\n",
      "Epoch: 0041 train_loss= 0.53863\n",
      "time= 0.01596\n",
      "Epoch: 0061 train_loss= 0.49334\n",
      "time= 0.02094\n",
      "Epoch: 0081 train_loss= 0.47483\n",
      "time= 0.01596\n",
      "ACC=0.608936, f1_macro=0.567735, precision_macro=0.579074, recall_macro=0.588276, f1_micro=0.608936, precision_micro=0.608936, recall_micro=0.608936, NMI=0.493074, ADJ_RAND_SCORE=0.416182\n",
      "Seed 2\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.77400\n",
      "time= 0.02094\n",
      "Epoch: 0021 train_loss= 0.68116\n",
      "time= 0.01695\n",
      "Epoch: 0041 train_loss= 0.53449\n",
      "time= 0.01695\n",
      "Epoch: 0061 train_loss= 0.47655\n",
      "time= 0.01695\n",
      "Epoch: 0081 train_loss= 0.45413\n",
      "time= 0.01496\n",
      "ACC=0.664697, f1_macro=0.639627, precision_macro=0.647936, recall_macro=0.656312, f1_micro=0.664697, precision_micro=0.664697, recall_micro=0.664697, NMI=0.495425, ADJ_RAND_SCORE=0.430237\n",
      "Seed 3\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.76678\n",
      "time= 0.03690\n",
      "Epoch: 0021 train_loss= 0.68139\n",
      "time= 0.01795\n",
      "Epoch: 0041 train_loss= 0.54786\n",
      "time= 0.02892\n",
      "Epoch: 0061 train_loss= 0.50026\n",
      "time= 0.02992\n",
      "Epoch: 0081 train_loss= 0.47743\n",
      "time= 0.02992\n",
      "ACC=0.652511, f1_macro=0.633217, precision_macro=0.638304, recall_macro=0.672365, f1_micro=0.652511, precision_micro=0.652511, recall_micro=0.652511, NMI=0.468910, ADJ_RAND_SCORE=0.416346\n",
      "Seed 4\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.76226\n",
      "time= 0.01696\n",
      "Epoch: 0021 train_loss= 0.64666\n",
      "time= 0.02194\n",
      "Epoch: 0041 train_loss= 0.51168\n",
      "time= 0.03291\n",
      "Epoch: 0061 train_loss= 0.45854\n",
      "time= 0.01895\n",
      "Epoch: 0081 train_loss= 0.43783\n",
      "time= 0.02992\n",
      "ACC=0.682053, f1_macro=0.659229, precision_macro=0.670620, recall_macro=0.686340, f1_micro=0.682053, precision_micro=0.682053, recall_micro=0.682053, NMI=0.484729, ADJ_RAND_SCORE=0.454655\n",
      "Seed 5\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.76925\n",
      "time= 0.03191\n",
      "Epoch: 0021 train_loss= 0.65591\n",
      "time= 0.02992\n",
      "Epoch: 0041 train_loss= 0.51462\n",
      "time= 0.01596\n",
      "Epoch: 0061 train_loss= 0.46443\n",
      "time= 0.01895\n",
      "Epoch: 0081 train_loss= 0.44278\n",
      "time= 0.02693\n",
      "ACC=0.638848, f1_macro=0.617380, precision_macro=0.626964, recall_macro=0.643421, f1_micro=0.638848, precision_micro=0.638848, recall_micro=0.638848, NMI=0.469058, ADJ_RAND_SCORE=0.422237\n",
      "Seed 6\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.76438\n",
      "time= 0.02493\n",
      "Epoch: 0021 train_loss= 0.64353\n",
      "time= 0.03092\n",
      "Epoch: 0041 train_loss= 0.50051\n",
      "time= 0.02992\n",
      "Epoch: 0061 train_loss= 0.47273\n",
      "time= 0.02992\n",
      "Epoch: 0081 train_loss= 0.45449\n",
      "time= 0.02194\n",
      "ACC=0.721196, f1_macro=0.702950, precision_macro=0.693373, recall_macro=0.734620, f1_micro=0.721196, precision_micro=0.721196, recall_micro=0.721196, NMI=0.536972, ADJ_RAND_SCORE=0.495869\n",
      "Seed 7\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.76767\n",
      "time= 0.02194\n",
      "Epoch: 0021 train_loss= 0.66824\n",
      "time= 0.03090\n",
      "Epoch: 0041 train_loss= 0.52523\n",
      "time= 0.04189\n",
      "Epoch: 0061 train_loss= 0.47333\n",
      "time= 0.03420\n",
      "Epoch: 0081 train_loss= 0.45656\n",
      "time= 0.03191\n",
      "ACC=0.666544, f1_macro=0.646103, precision_macro=0.679441, recall_macro=0.678840, f1_micro=0.666544, precision_micro=0.666544, recall_micro=0.666544, NMI=0.483992, ADJ_RAND_SCORE=0.433184\n",
      "Seed 8\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.76733\n",
      "time= 0.02094\n",
      "Epoch: 0021 train_loss= 0.63708\n",
      "time= 0.01596\n",
      "Epoch: 0041 train_loss= 0.51861\n",
      "time= 0.02992\n",
      "Epoch: 0061 train_loss= 0.48505\n",
      "time= 0.03092\n",
      "Epoch: 0081 train_loss= 0.46394\n",
      "time= 0.02992\n",
      "ACC=0.652880, f1_macro=0.630524, precision_macro=0.641520, recall_macro=0.660403, f1_micro=0.652880, precision_micro=0.652880, recall_micro=0.652880, NMI=0.473266, ADJ_RAND_SCORE=0.437120\n",
      "Seed 9\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 0.76189\n",
      "time= 0.03591\n",
      "Epoch: 0021 train_loss= 0.67943\n",
      "time= 0.02992\n",
      "Epoch: 0041 train_loss= 0.52756\n",
      "time= 0.02194\n",
      "Epoch: 0061 train_loss= 0.47864\n",
      "time= 0.01695\n",
      "Epoch: 0081 train_loss= 0.46410\n",
      "time= 0.01895\n",
      "ACC=0.660266, f1_macro=0.639605, precision_macro=0.650197, recall_macro=0.673200, f1_micro=0.660266, precision_micro=0.660266, recall_micro=0.660266, NMI=0.493433, ADJ_RAND_SCORE=0.444586\n",
      "acc = 0.6601181683899556 , std =  0.02739233143408342\n",
      "nmi = 0.4897500152505949 , std =  0.01886364031656808\n",
      "f1_macro = 0.6366324764564399 , std =  0.03197977220411297\n",
      "precision_macro = 0.6472487799009788 , std =  0.029826303541492086\n",
      "adjscore = 0.43802012000815954 , std =  0.02239350474931951\n"
     ]
    }
   ],
   "source": [
    "once = False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if once == True:\n",
    "        gae_for(args)\n",
    "    else:\n",
    "        clustering_scores = []\n",
    "        clustering_metrics_names = ['acc', 'nmi', 'f1_macro', 'precision_macro', 'adjscore']\n",
    "        \n",
    "        # using 10 different random seeds\n",
    "        for seed in range(10):\n",
    "            print('Seed',seed)\n",
    "            args.seed = seed\n",
    "            torch.manual_seed(args.seed)\n",
    "            clustering_score = gae_for(args)\n",
    "            clustering_scores.append(clustering_score)\n",
    "        # show the results by mean and std\n",
    "        clustering_scores = np.asarray(clustering_scores)\n",
    "        for i in range(len(clustering_scores[0])):\n",
    "            print(clustering_metrics_names[i],'=',np.mean(clustering_scores[:,i]),', std = ',np.std(clustering_scores[:,i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
