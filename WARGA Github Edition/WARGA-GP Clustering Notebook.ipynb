{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from model import GCNModelAE, Regularizer\n",
    "from optimizer import loss_function1\n",
    "from utils import load_data, preprocess_graph, get_roc_score, load_data_with_labels\n",
    "from sklearn.cluster import KMeans\n",
    "from metrics import clustering_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Settings\n",
    "\n",
    "Here in node clustering we only use half of the training iterations for link prediction (i.e. 100 epochs for Cora and Citeseer, and 750 epochs for PubMed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
    "# We recommend 100 epochs for Cora and Citeseer, and 800 epochs for PubMed\n",
    "parser.add_argument('--hidden1', type=int, default=32, help='Number of units in the first encoding layer.')\n",
    "parser.add_argument('--hidden2', type=int, default=16, help='Number of units in the second embedding layer.')\n",
    "parser.add_argument('--hidden3', type=int, default=16, help='Number of units in the first hidden layer of Regularizer.')\n",
    "parser.add_argument('--hidden4', type=int, default=64, help='Number of units in the second hidden layer of Regularizer.')\n",
    "parser.add_argument('--gp_lambda', type=float, default=10.0, help='lambda for gradient penalty.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate for Generator.')\n",
    "parser.add_argument('--reglr', type=float, default=0.001, help='Initial learning rate for Regularizer.')\n",
    "parser.add_argument('--dropout', type=float, default=0., help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--dataset-str', type=str, default='citeseer', help='type of dataset.')\n",
    "\n",
    "args,unknown = parser.parse_known_args()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)    \n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Node Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cited from Improved Training of Wasserstein GANs \n",
    "# https://github.com/igul222/improved_wgan_training\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(outputs=d_interpolates,\n",
    "                              inputs=interpolates,\n",
    "                              grad_outputs=fake,\n",
    "                              create_graph=True,\n",
    "                              retain_graph=True,\n",
    "                              only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 0.01) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "def gae_for(args):\n",
    "    print(\"Using {} dataset\".format(args.dataset_str))\n",
    "    adj, features,true_labels = load_data_with_labels(args.dataset_str)\n",
    "    n_nodes, feat_dim = features.shape\n",
    "    features = features.to(device)\n",
    "    \n",
    "    if args.dataset_str == 'cora':\n",
    "        n_clusters = 7\n",
    "    elif args.dataset_str == 'citeseer':\n",
    "        n_clusters = 6\n",
    "    else:\n",
    "        n_clusters = 3\n",
    "\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "\n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_norm = adj_norm.to(device)\n",
    "    \n",
    "    adj_label = adj + sp.eye(adj.shape[0])\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    adj_label = adj_label.to(device)\n",
    "\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "    model = GCNModelAE(feat_dim, args.hidden1, args.hidden2, args.dropout).to(device)\n",
    "    regularizer = Regularizer(args.hidden3, args.hidden2, args.hidden4).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    regularizer_optimizer = optim.Adam(regularizer.parameters(), lr=args.reglr)\n",
    "    \n",
    "    clustering_scores=[]\n",
    "    for epoch in range(args.epochs):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        regularizer.train() \n",
    "        \n",
    "        #Generate embeddings\n",
    "        predicted_labels_prob, emb = model(features, adj_norm)\n",
    "        \n",
    "        #Wasserstein Regularizer\n",
    "        for i in range(1):\n",
    "            f_z = regularizer(emb).to(device)\n",
    "            r = torch.normal(0.0, 1.0, [n_nodes, args.hidden2]).to(device)\n",
    "            f_r = regularizer(r)  \n",
    "            \n",
    "            # add the gradient penalty to objective function\n",
    "            gradient_penalty = compute_gradient_penalty(regularizer, r, emb)\n",
    "            \n",
    "            reg_loss = - f_r.mean() + f_z.mean() + args.gp_lambda * gradient_penalty\n",
    "            \n",
    "            regularizer_optimizer.zero_grad()\n",
    "            reg_loss.backward(retain_graph=True)\n",
    "            regularizer_optimizer.step()\n",
    "            \n",
    "        \n",
    "        #GAE Update\n",
    "        f_z = regularizer(emb)  \n",
    "        generator_loss = -f_z.mean()\n",
    "        loss = loss_function1(preds=predicted_labels_prob, labels=adj_label,\n",
    "                             norm=norm, pos_weight=torch.tensor(pos_weight))\n",
    "        loss = loss + generator_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()\n",
    "        if epoch%20==0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss))\n",
    "            print(\"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        \n",
    "    np_emb = emb.cpu().detach().numpy()\n",
    "    kmeans = KMeans(n_clusters= n_clusters, random_state=args.seed).fit(np_emb)\n",
    "    predict_labels = kmeans.predict(np_emb)\n",
    "    cm = clustering_metrics(true_labels, predict_labels)\n",
    "    acc, nmi, f1_macro, precision_macro, adjscore = cm.evaluationClusterModelFromLabel()\n",
    "\n",
    "    clustering_scores.append([acc, nmi, f1_macro, precision_macro, adjscore])\n",
    " \n",
    "    return clustering_scores[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80502\n",
      "time= 0.66223\n",
      "Epoch: 0021 train_loss= 0.59620\n",
      "time= 0.03989\n",
      "Epoch: 0041 train_loss= 0.54817\n",
      "time= 0.03391\n",
      "Epoch: 0061 train_loss= 0.54077\n",
      "time= 0.02493\n",
      "Epoch: 0081 train_loss= 0.55468\n",
      "time= 0.03092\n",
      "ACC=0.555756, f1_macro=0.545880, precision_macro=0.578855, recall_macro=0.540565, f1_micro=0.555756, precision_micro=0.555756, recall_micro=0.555756, NMI=0.299578, ADJ_RAND_SCORE=0.271206\n",
      "Seed 1\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80852\n",
      "time= 0.03490\n",
      "Epoch: 0021 train_loss= 0.61043\n",
      "time= 0.03989\n",
      "Epoch: 0041 train_loss= 0.57503\n",
      "time= 0.03092\n",
      "Epoch: 0061 train_loss= 0.55335\n",
      "time= 0.02793\n",
      "Epoch: 0081 train_loss= 0.55824\n",
      "time= 0.03291\n",
      "ACC=0.594530, f1_macro=0.577767, precision_macro=0.611843, recall_macro=0.583218, f1_micro=0.594530, precision_micro=0.594530, recall_micro=0.594530, NMI=0.337386, ADJ_RAND_SCORE=0.312114\n",
      "Seed 2\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80303\n",
      "time= 0.03491\n",
      "Epoch: 0021 train_loss= 0.60530\n",
      "time= 0.02992\n",
      "Epoch: 0041 train_loss= 0.52906\n",
      "time= 0.02992\n",
      "Epoch: 0061 train_loss= 0.51009\n",
      "time= 0.03291\n",
      "Epoch: 0081 train_loss= 0.51296\n",
      "time= 0.03491\n",
      "ACC=0.554253, f1_macro=0.537192, precision_macro=0.566905, recall_macro=0.526771, f1_micro=0.554253, precision_micro=0.554253, recall_micro=0.554253, NMI=0.296232, ADJ_RAND_SCORE=0.281295\n",
      "Seed 3\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80776\n",
      "time= 0.03690\n",
      "Epoch: 0021 train_loss= 0.58483\n",
      "time= 0.03092\n",
      "Epoch: 0041 train_loss= 0.53316\n",
      "time= 0.03590\n",
      "Epoch: 0061 train_loss= 0.51821\n",
      "time= 0.03491\n",
      "Epoch: 0081 train_loss= 0.53026\n",
      "time= 0.04089\n",
      "ACC=0.553652, f1_macro=0.526666, precision_macro=0.545859, recall_macro=0.522753, f1_micro=0.553652, precision_micro=0.553652, recall_micro=0.553652, NMI=0.296733, ADJ_RAND_SCORE=0.275625\n",
      "Seed 4\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80286\n",
      "time= 0.04388\n",
      "Epoch: 0021 train_loss= 0.57399\n",
      "time= 0.03191\n",
      "Epoch: 0041 train_loss= 0.51819\n",
      "time= 0.03092\n",
      "Epoch: 0061 train_loss= 0.51168\n",
      "time= 0.02992\n",
      "Epoch: 0081 train_loss= 0.52255\n",
      "time= 0.03391\n",
      "ACC=0.596333, f1_macro=0.570837, precision_macro=0.589622, recall_macro=0.564513, f1_micro=0.596333, precision_micro=0.596333, recall_micro=0.596333, NMI=0.327316, ADJ_RAND_SCORE=0.328178\n",
      "Seed 5\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80626\n",
      "time= 0.03890\n",
      "Epoch: 0021 train_loss= 0.52901\n",
      "time= 0.02992\n",
      "Epoch: 0041 train_loss= 0.41769\n",
      "time= 0.02194\n",
      "Epoch: 0061 train_loss= 0.42099\n",
      "time= 0.02194\n",
      "Epoch: 0081 train_loss= 0.43558\n",
      "time= 0.02194\n",
      "ACC=0.524196, f1_macro=0.510815, precision_macro=0.535685, recall_macro=0.505534, f1_micro=0.524196, precision_micro=0.524196, recall_micro=0.524196, NMI=0.271900, ADJ_RAND_SCORE=0.250339\n",
      "Seed 6\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.81446\n",
      "time= 0.02493\n",
      "Epoch: 0021 train_loss= 0.63215\n",
      "time= 0.02194\n",
      "Epoch: 0041 train_loss= 0.63324\n",
      "time= 0.02094\n",
      "Epoch: 0061 train_loss= 0.61699\n",
      "time= 0.02294\n",
      "Epoch: 0081 train_loss= 0.63927\n",
      "time= 0.02194\n",
      "ACC=0.522092, f1_macro=0.495326, precision_macro=0.517678, recall_macro=0.485311, f1_micro=0.522092, precision_micro=0.522092, recall_micro=0.522092, NMI=0.262186, ADJ_RAND_SCORE=0.247760\n",
      "Seed 7\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80173\n",
      "time= 0.02094\n",
      "Epoch: 0021 train_loss= 0.49151\n",
      "time= 0.02094\n",
      "Epoch: 0041 train_loss= 0.33029\n",
      "time= 0.02194\n",
      "Epoch: 0061 train_loss= 0.36491\n",
      "time= 0.02493\n",
      "Epoch: 0081 train_loss= 0.34955\n",
      "time= 0.02493\n",
      "ACC=0.588819, f1_macro=0.566963, precision_macro=0.585083, recall_macro=0.564535, f1_micro=0.588819, precision_micro=0.588819, recall_micro=0.588819, NMI=0.320028, ADJ_RAND_SCORE=0.313902\n",
      "Seed 8\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80465\n",
      "time= 0.02593\n",
      "Epoch: 0021 train_loss= 0.58032\n",
      "time= 0.02094\n",
      "Epoch: 0041 train_loss= 0.50357\n",
      "time= 0.02194\n",
      "Epoch: 0061 train_loss= 0.49844\n",
      "time= 0.02294\n",
      "Epoch: 0081 train_loss= 0.50832\n",
      "time= 0.02094\n",
      "ACC=0.541028, f1_macro=0.529714, precision_macro=0.562706, recall_macro=0.519249, f1_micro=0.541028, precision_micro=0.541028, recall_micro=0.541028, NMI=0.292819, ADJ_RAND_SCORE=0.265299\n",
      "Seed 9\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 0.80427\n",
      "time= 0.02693\n",
      "Epoch: 0021 train_loss= 0.56199\n",
      "time= 0.02294\n",
      "Epoch: 0041 train_loss= 0.52728\n",
      "time= 0.02094\n",
      "Epoch: 0061 train_loss= 0.52820\n",
      "time= 0.02194\n",
      "Epoch: 0081 train_loss= 0.54852\n",
      "time= 0.02194\n",
      "ACC=0.600541, f1_macro=0.577003, precision_macro=0.593088, recall_macro=0.574820, f1_micro=0.600541, precision_micro=0.600541, recall_micro=0.600541, NMI=0.315928, ADJ_RAND_SCORE=0.317388\n",
      "acc = 0.5631199278629395 , std =  0.028389014564310797\n",
      "nmi = 0.3020105862641918 , std =  0.022482051482407995\n",
      "f1_macro = 0.5438162684720728 , std =  0.027388654923924744\n",
      "precision_macro = 0.5687323422858009 , std =  0.027421452066276535\n",
      "adjscore = 0.28631053296698317 , std =  0.02781172372404818\n"
     ]
    }
   ],
   "source": [
    "once = False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if once == True:\n",
    "        gae_for(args)\n",
    "    else:\n",
    "        clustering_scores = []\n",
    "        clustering_metrics_names = ['acc', 'nmi', 'f1_macro', 'precision_macro', 'adjscore']\n",
    "        \n",
    "        # using 10 different random seeds\n",
    "        for seed in range(10):\n",
    "            print('Seed',seed)\n",
    "            args.seed = seed\n",
    "            torch.manual_seed(args.seed)\n",
    "            clustering_score = gae_for(args)\n",
    "            clustering_scores.append(clustering_score)\n",
    "        # show the results by mean and std\n",
    "        clustering_scores = np.asarray(clustering_scores)\n",
    "        for i in range(len(clustering_scores[0])):\n",
    "            print(clustering_metrics_names[i],'=',np.mean(clustering_scores[:,i]),', std = ',np.std(clustering_scores[:,i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
